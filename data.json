{
  "flashcards": [
    {"q":"What is the Mahalanobis distance?","a":"The squared distance $(x-\\mu)^{\\top}\\Sigma^{-1}(x-\\mu)$ that rescales and decorrelates space using the covariance matrix."},
    {"q":"What condition on eigenvalues ensures a symmetric matrix is positive definite?","a":"All eigenvalues are strictly greater than zero."},
    {"q":"Write the pdf of the $N_p(\\mu,\\Sigma)$ distribution.","a":"$(2\\pi)^{-p/2}|\\Sigma|^{-1/2}\\,\\exp\\bigl[-\\tfrac12(x-\\mu)^{\\top}\\Sigma^{-1}(x-\\mu)\\bigr]$."},
    {"q":"What is Hotelling’s $T^2$ test used for?","a":"Testing hypotheses about one or two multivariate mean vectors."},
    {"q":"Which two sums-of-squares matrices form the basis of MANOVA?","a":"The hypothesis matrix $H$ and the error matrix $E$."},
    {"q":"Define Wilks’ Lambda.","a":"$\\Lambda = |E|\\big/|E+H|$, measuring the proportion of unexplained variance."},
    {"q":"How is the unbiased sample covariance matrix calculated?","a":"$S = \\dfrac{1}{n-1}(X-\\bar X\\mathbf1^{\\top})(X-\\bar X\\mathbf1^{\\top})^{\\top}$."},
    {"q":"When are $\\bar X$ and $S$ independent?","a":"When the data come from a multivariate normal distribution."},
    {"q":"What does Mardia’s skewness statistic test?","a":"Departure from multivariate normality in terms of skewness."},
    {"q":"What is a principal component?","a":"A linear combination of variables maximising variance subject to orthogonality constraints."},
    {"q":"State the Kaiser criterion for choosing PCs.","a":"Retain components whose eigenvalues exceed the average eigenvalue."},
    {"q":"What null hypothesis is tested by Box’s $M$?","a":"Equality of the covariance matrices across several groups."},
    {"q":"Which key assumption differentiates LDA from QDA?","a":"LDA assumes a common covariance matrix for all classes; QDA does not."},
    {"q":"What is the Wishart distribution?","a":"The distribution of $ (n-1)S$ when sampling from $N_p(\\mu,\\Sigma)$."},
    {"q":"What matrix projects any vector onto the column space of $A$?","a":"$P_A = A(A^{\\top}A)^{-1}A^{\\top}$, the orthogonal projector."},
    {"q":"Why are eigenvalues important in PCA?","a":"They quantify the variance captured by each principal component."},
    {"q":"Define Pillai’s trace.","a":"$V = \\operatorname{tr}\\bigl[H(H+E)^{-1}\\bigr]$, an alternative MANOVA statistic."},
    {"q":"What is canonical correlation analysis used for?","a":"Finding maximally correlated linear combinations of two variable sets."},
    {"q":"In multivariate multiple regression, how many dependent variables can be modelled?","a":"Two or more simultaneously."},
    {"q":"What is the purpose of a Q–Q plot of Mahalanobis distances?","a":"Graphically assessing multivariate normality."},
    {"q":"Give another name for the residual-maker matrix in regression.","a":"$Q = I - P$, where $P$ is the projector onto the design matrix."},
    {"q":"What distribution does $\\frac{p(n-1)}{n-p}d^2$ follow under $H_0$?","a":"$F_{p,\\,n-p}$."},
    {"q":"Which PCA rule keeps components explaining at least 80 % variance?","a":"The cumulative variance rule (variance-explained threshold)."},
    {"q":"What does the scree plot ‘elbow’ indicate?","a":"A natural cut-off point beyond which adding PCs yields diminishing returns."},
    {"q":"How many linear discriminant functions can be extracted for $g$ groups?","a":"$\\min(p,\\,g-1)$."},
    {"q":"What is Roy’s largest root?","a":"The maximal eigenvalue of $E^{-1}H$ in MANOVA."},
    {"q":"What is the determinant of a correlation matrix always equal to or greater than?","a":"Zero (it is non-negative)."},
    {"q":"What is Mardia’s kurtosis expected value under normality?","a":"$p(p+2)$."},
    {"q":"What is the dimension of $E$ and $H$ if there are $p$ variables?","a":"Both are $p\\times p$."},
    {"q":"How is the pooled covariance matrix defined in two-sample $T^2$?","a":"$S_p = \\dfrac{(n_1-1)S_1+(n_2-1)S_2}{n_1+n_2-2}$."}
  ],

  "multipleChoice": [
    {"question":"Which statistic is *not* used as a MANOVA omnibus test?","choices":["Wilks’ $\\Lambda$","Pillai’s trace","Hotelling–Lawley trace","Mahalanobis distance"],"answer":"Mahalanobis distance"},
    {"question":"The eigenvectors of which matrix give principal components?","choices":["$S$","$S^{-1}$","$E^{-1}H$","$X^{\\top}X$"],"answer":"$S$"},
    {"question":"If $\\Lambda$ in MANOVA is very small, what is the likely decision?","choices":["Fail to reject $H_0$","Reject $H_0$","Increase sample size","Use a different test"],"answer":"Reject $H_0$"},
    {"question":"Hotelling’s $T^2$ converts to an $F$ distribution with numerator df equal to:","choices":["$n$","$p$","$n-p$","$p(n-1)$"],"answer":"$p$"},
    {"question":"Which plot best assesses multivariate normality?","choices":["Pairs scatterplot","Histogram of one variable","Mahalanobis Q–Q plot","Scree plot"],"answer":"Mahalanobis Q–Q plot"},
    {"question":"When computing PCA on variables of different scales, you should use:","choices":["The raw covariance matrix","The correlation matrix","The pooled covariance","The residual matrix"],"answer":"The correlation matrix"},
    {"question":"Box’s $M$ statistic asymptotically follows which distribution (after correction)?","choices":["$F$","Chi-square","Normal","t"],"answer":"Chi-square"},
    {"question":"LDA classifies an observation by choosing the class with:","choices":["Largest Mahalanobis distance","Smallest discriminant score","Largest discriminant score","Smallest group mean"],"answer":"Largest discriminant score"},
    {"question":"Pillai’s trace is equal to:","choices":["Product of $\\lambda_i$","Sum of $\\lambda_i$","Max of $\\lambda_i$","Determinant of $H$"],"answer":"Sum of $\\lambda_i$"},
    {"question":"The Wishart distribution is a multivariate analogue of which univariate distribution?","choices":["t","Normal","Chi-square","F"],"answer":"Chi-square"},
    {"question":"Which matrix is idempotent?","choices":["$S$","$P_A$","$E^{-1}H$","$\\Sigma$"],"answer":"$P_A$"},
    {"question":"In PCA, which component explains the *least* variance?","choices":["First","Second","Last","They all equal"],"answer":"Last"}
  ],

  "writtenAnswer": [
    {"q":"Derive the transformation from Hotelling’s $T^2$ to an $F$ distribution in the one-sample case.","a":"Use $F=\\dfrac{n-p}{p(n-1)}T^2$, which under $H_0$ follows $F_{p,\\,n-p}$ by properties of the Wishart distribution."},
    {"q":"Explain why $\\bar X$ and $S$ are independent only when sampling from a multivariate normal distribution.","a":"Because the joint likelihood factorises into independent functions of $\\bar X$ and centred data only under normality; otherwise dependence remains."},
    {"q":"State and interpret Wilks’ $\\Lambda$ in MANOVA.","a":"$\\Lambda=|E|/|E+H|$; values near 1 indicate group means are similar, whereas values near 0 indicate at least one mean differs."},
    {"q":"Describe two graphical methods to check multivariate normality.","a":"(1) Q–Q plot of squared Mahalanobis distances vs $\\chi^2_p$ quantiles; (2) chi-square plot of ordered distances with reference line."},
    {"q":"Outline the steps to compute the first two principal components from a data matrix.","a":"Center data → compute covariance (or correlation) → eigen-decompose → sort eigenvalues → take eigenvectors for largest two → project data to obtain scores."},
    {"q":"Give two reasons for standardising variables before PCA.","a":"(1) Remove scale differences so each variable contributes equally; (2) prevent variables with large variance from dominating components."},
    {"q":"What is the null hypothesis in Box’s $M$ and why does its violation matter for LDA?","a":"$H_0: \\Sigma_1=\\dots=\\Sigma_g$; if violated, common-covariance assumption fails, so LDA is sub-optimal and QDA preferred."},
    {"q":"Define the error and hypothesis matrices in a one-way MANOVA.","a":"$E$ sums within-group SSCPs; $H$ sums squared deviations of group means from the grand mean multiplied by group sizes."},
    {"q":"Explain how canonical variates are obtained in canonical correlation analysis.","a":"Solve the eigenproblem of $\\Sigma_{XX}^{-1}\\Sigma_{XY}\\Sigma_{YY}^{-1}\\Sigma_{YX}$ to find weight vectors maximising cross-correlation."},
    {"q":"Why might Pillai’s trace be preferred to Wilks’ $\\Lambda$?","a":"It is more robust to violations of assumptions, especially unequal covariance matrices and small sample sizes."}
  ],

  "trueFalse": [
    {"q":"Mahalanobis distance accounts for correlations among variables.",              "a": true},
    {"q":"If all pairwise correlations are zero, the correlation matrix is the identity.", "a": true},
    {"q":"A covariance matrix can have negative eigenvalues.",                           "a": false},
    {"q":"The determinant of $E^{-1}H$ equals Wilks’ $\\Lambda$.",                       "a": false},
    {"q":"Mardia’s kurtosis equals $p(p+2)$ under multivariate normality.",              "a": true},
    {"q":"In PCA, eigenvectors corresponding to smaller eigenvalues capture more variance.", "a": false},
    {"q":"Hotelling’s $T^2$ reduces to the univariate $t$-test when $p=1$.",             "a": true},
    {"q":"Box’s $M$ is insensitive to departures from multivariate normality.",          "a": false},
    {"q":"LDA assumes equal covariance matrices for all classes.",                       "a": true},
    {"q":"Pillai’s trace is bounded above by the number of dependent variables.",        "a": true},
    {"q":"An idempotent matrix satisfies $P^2=P$.",                                      "a": true},
    {"q":"The sample covariance matrix $S$ is always positive semi-definite.",           "a": true},
    {"q":"Wilks’ $\\Lambda$ approaches 1 when group means are very different.",          "a": false},
    {"q":"Canonical correlations are always less than or equal to 1.",                   "a": true},
    {"q":"In MANOVA, $E$ captures between-group variability.",                           "a": false},
    {"q":"Principal components are uncorrelated by construction.",                       "a": true},
    {"q":"The Wishart distribution generalises the $F$ distribution.",                   "a": false},
    {"q":"QDA is preferred when Box’s $M$ rejects equality of covariances.",             "a": true},
    {"q":"PCA can be applied to categorical data without modification.",                 "a": false},
    {"q":"The residual-maker matrix $Q$ is idempotent.",                                 "a": true}
  ]
}